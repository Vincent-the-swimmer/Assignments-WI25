{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.racetrack import RaceTrack, tiny_course, big_course\n",
    "from src.montecarlo import MonteCarloControl\n",
    "from src.dp import DynamicProgramming\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Dynamic Programming on the Racecar Environment\n",
    "\n",
    "For this part of the assignment, implement both Policy Iteration and Value Iteration in `src/dp.py` to solve the Racecar environment using Dynamic Programming. In this part of the notebook, we will vidualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_track(track, states=None):\n",
    "    \"\"\"\n",
    "    Visualizes the racetrack and the trajectory of the car.\n",
    "    \n",
    "    Args:\n",
    "        track: RaceTrack environment.\n",
    "        states: List of state tuples (x, y, vx, vy) representing the trajectory.\n",
    "    \"\"\"\n",
    "    course_layout = track.course.copy()\n",
    "\n",
    "    if states is None:\n",
    "        states = [tuple(track.get_state())]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.flipud(course_layout.T), cmap=\"hot\", interpolation=\"nearest\")\n",
    "    plt.title(\"Policy Visualization - Marker shows position, arrow shows velocity\")\n",
    "\n",
    "    for state in states:\n",
    "        x, y, vx, vy = state\n",
    "        y = course_layout.shape[1] - 1 - y  # Flip vertically to match plot orientation\n",
    "        plt.plot(x, y, \"bo\", markersize=10)\n",
    "        plt.arrow(x, y, vx, -vy, color=\"cyan\", head_width=0.2, length_includes_head=True, zorder=100)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def simulate_policy(env, policy, max_steps=100):\n",
    "    \"\"\"\n",
    "    Simulates the racetrack environment using a given policy.\n",
    "    \n",
    "    Args:\n",
    "        env: RaceTrack environment.\n",
    "        policy: Optimized policy from Dynamic Programming.\n",
    "        max_steps: Maximum number of steps to simulate.\n",
    "    \n",
    "    Returns:\n",
    "        List of visited states.\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    trajectory = [tuple(env.get_state())]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        state = tuple(env.get_state())\n",
    "        if state not in policy:\n",
    "            break\n",
    "        action = int(policy[state])\n",
    "        env.take_action(action)\n",
    "        trajectory.append(tuple(env.get_state()))\n",
    "\n",
    "        if env.is_terminal_state():\n",
    "            break\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "# Run Dynamic Programming for both methods\n",
    "env = RaceTrack(tiny_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below will run your Policy Iteration implementation on the Racecar environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Policy Iteration\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dp_solver_pi \u001b[38;5;241m=\u001b[39m DynamicProgramming(env)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdp_solver_pi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolicy_iteration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m trajectory_pi \u001b[38;5;241m=\u001b[39m simulate_policy(env, dp_solver_pi\u001b[38;5;241m.\u001b[39mpolicy)\n\u001b[0;32m      5\u001b[0m plot_track(env, trajectory_pi)\n",
      "File \u001b[1;32mc:\\Users\\yozyg\\OneDrive\\Documents\\GitHub\\Assignments-WI25\\A3\\src\\dp.py:212\u001b[0m, in \u001b[0;36mDynamicProgramming.solve\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03mSolve the environment using the specified DP method.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_iteration\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue_iteration\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_iteration()\n",
      "File \u001b[1;32mc:\\Users\\yozyg\\OneDrive\\Documents\\GitHub\\Assignments-WI25\\A3\\src\\dp.py:167\u001b[0m, in \u001b[0;36mDynamicProgramming.policy_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m policy_stable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m policy_stable: \n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     policy_stable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_improvement()\n",
      "File \u001b[1;32mc:\\Users\\yozyg\\OneDrive\\Documents\\GitHub\\Assignments-WI25\\A3\\src\\dp.py:91\u001b[0m, in \u001b[0;36mDynamicProgramming.policy_evaluation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     88\u001b[0m new_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     90\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy[state]  \u001b[38;5;66;03m# âœ… Get the single action for this state\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m reward, new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m new_value \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_function[new_state]  \u001b[38;5;66;03m# Update value function\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Update value function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yozyg\\OneDrive\\Documents\\GitHub\\Assignments-WI25\\A3\\src\\dp.py:197\u001b[0m, in \u001b[0;36mDynamicProgramming._simulate_action\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x, y])\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mvelocity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([vx, vy])\n\u001b[1;32m--> 197\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_state())\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward, new_state\n",
      "File \u001b[1;32mc:\\Users\\yozyg\\OneDrive\\Documents\\GitHub\\Assignments-WI25\\A3\\src\\racetrack.py:72\u001b[0m, in \u001b[0;36mRaceTrack.take_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_velocity(action)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yozyg\\OneDrive\\Documents\\GitHub\\Assignments-WI25\\A3\\src\\racetrack.py:110\u001b[0m, in \u001b[0;36mRaceTrack._update_position\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m t \u001b[38;5;241m=\u001b[39m tstep \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_VELOCITY\n\u001b[0;32m    109\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity \u001b[38;5;241m*\u001b[39m t)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_wall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_start_position()\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint16)\n",
      "File \u001b[1;32mc:\\Users\\yozyg\\OneDrive\\Documents\\GitHub\\Assignments-WI25\\A3\\src\\racetrack.py:149\u001b[0m, in \u001b[0;36mRaceTrack._is_wall\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcourse[x, y] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    147\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_positions\u001b[38;5;241m.\u001b[39mappend((x, y))\n\u001b[1;32m--> 149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_wall\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos):\n\u001b[0;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return True is position is wall\"\"\"\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcourse[pos[\u001b[38;5;241m0\u001b[39m], pos[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Policy Iteration\n",
    "dp_solver_pi = DynamicProgramming(env)\n",
    "dp_solver_pi.solve(method='policy_iteration')\n",
    "trajectory_pi = simulate_policy(env, dp_solver_pi.policy)\n",
    "plot_track(env, trajectory_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the snippet below will run your Value Iteration implementation on the Racecar environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration\n",
    "dp_solver_vi = DynamicProgramming(env)\n",
    "dp_solver_vi.solve(method='value_iteration')\n",
    "trajectory_vi = simulate_policy(env, dp_solver_vi.policy)\n",
    "plot_track(env, trajectory_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you may try this out on the larger racecar environments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Monte Carlo Control\n",
    "\n",
    "Let's collect episodes where we behave using $\\epsilon$-greedy policy and update a greedy policy using weighted importance sampling. \n",
    "\n",
    "We will use the code you wrote, and try to solve a small racecourse.  We will collect information about the behavior policy during learning, and at the end of learning we will collect a single episode per learner of the greedy target policy\n",
    "\n",
    "Be warned, running this took 8 minutes using my code solution on a 2019 Macbook 2.4 GHz 8 core i9 operating in a conda COGS188Wi25 environment. Obviously YMMV according to your code's efficiency and the hardware/system setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(619) # replicability \n",
    "\n",
    "steps = []\n",
    "behavior_rewards = []\n",
    "target_rewards = []\n",
    "for j in tqdm(range(20)):\n",
    "    MC = MonteCarloControl(RaceTrack(tiny_course), max_episode_size=250)\n",
    "    this_steps = []\n",
    "    this_rewards = []\n",
    "    for k in range(2000):\n",
    "        episode = MC.generate_egreedy_episode()\n",
    "        this_rewards.append( pd.DataFrame(episode).iloc[:,-1].sum() )\n",
    "        this_steps.append(len(episode))\n",
    "        MC.update_offpolicy(episode)\n",
    "    steps.append(this_steps)\n",
    "    behavior_rewards.append(this_rewards)\n",
    "    episode = MC.generate_greedy_episode()\n",
    "    target_rewards.append( pd.DataFrame(episode).iloc[:,-1].sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = pd.DataFrame(steps)\n",
    "df1.columns.name = 'episode'\n",
    "df1.index.name = 'learner'\n",
    "df1 = df1.stack().rename('steps').to_frame()\n",
    "\n",
    "sns.lineplot( data=df1, x='episode', y='steps');\n",
    "plt.title('The mean step length of an episode goes down with episode number')\n",
    "plt.show()\n",
    "print('Welp at least there is some indication of learning')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.DataFrame(behavior_rewards)\n",
    "df2.columns.name = 'episode'\n",
    "df2.index.name = 'learner'\n",
    "df2 = df2.stack().rename('returns').to_frame()\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame(target_rewards)\n",
    "df3.columns.name = 'episode'\n",
    "df3.index.name = 'learner'\n",
    "df3 = df3.stack().rename('returns').to_frame()\n",
    "\n",
    "sns.lineplot( data=df2, x='episode', y='returns');\n",
    "plt.title('The mean reward of an episode goes up with episode number')\n",
    "plt.show()\n",
    "print('Welp at least there is some indication of learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df2.query('episode==999').reset_index().drop(columns=['episode'])\n",
    "c1['condition']='behavior policy'\n",
    "c2 = df3.droplevel('episode').reset_index()\n",
    "c2['condition']='target policy'\n",
    "\n",
    "compare = pd.concat( [c1, c2], ignore_index=True)\n",
    "sns.boxplot(data=compare, x='condition', y='returns');\n",
    "plt.title('Target greedy policy is median better and\\nmuch less variable than e-greedy behavior policy');\n",
    "\n",
    "\n",
    "def IQR(column): \n",
    "    q25, q75 = column.quantile([0.25, 0.75])\n",
    "    return q75-q25\n",
    "\n",
    "stats_list = [\n",
    "    'min', 'median', 'max', IQR  \n",
    "]\n",
    "\n",
    "print(compare.drop(columns=['learner']).groupby('condition').agg(stats_list))\n",
    "plt.show()\n",
    "print('We can also see that (at least at the current number of episodes) even greedy sometimes is still failing hard')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the bigger course\n",
    "Lets just run the bigger racecourse for longer because we know this is going to take longer to learn due to the larger state space.  At first I tried single runs out to >20k episodes and it still wasn't working until I tried a trick...\n",
    "\n",
    "A Q0=-50 is helpful because it encourages the system to learn more quickly... Any return that is better than 50 moves looks like a real good deal! This is the opposite of a positive Q0 that encourages exploration, here we ask the MC to lock in quickly and exploit as soon as it finds a somewhat OK solution.  With that mod in place we finally get reasonable learning in reasonable time.  Without it (feel free to try!) I just ran out of patience and time.\n",
    "\n",
    "But note that while we've found an OK solution already, its likely that a much better unfound solution still exists, and would obviously take >>20k episodes to learn with this kind of e-greedy exploration to randomly find this better solution\n",
    "\n",
    "Be warned even with this trick this run took 9 minutes to run on my Macbook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) # replicability \n",
    "\n",
    "steps = []\n",
    "behavior_rewards = []\n",
    "target_rewards = []\n",
    "for j in tqdm(range(20)):\n",
    "    MC = MonteCarloControl(RaceTrack(big_course), max_episode_size=500, Q0=-50)\n",
    "    this_steps = []\n",
    "    this_rewards = []\n",
    "    for k in range(2500):\n",
    "        episode = MC.generate_egreedy_episode()\n",
    "        this_rewards.append( pd.DataFrame(episode).iloc[:,-1].sum() )\n",
    "        this_steps.append(len(episode))\n",
    "        MC.update_offpolicy(episode)\n",
    "    steps.append(this_steps)\n",
    "    behavior_rewards.append(this_rewards)\n",
    "    episode = MC.generate_greedy_episode()\n",
    "    target_rewards.append( pd.DataFrame(episode).iloc[:,-1].sum() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.DataFrame(behavior_rewards)\n",
    "df2.columns.name = 'episode'\n",
    "df2.index.name = 'learner'\n",
    "df2 = df2.stack().rename('returns').to_frame()\n",
    "\n",
    "\n",
    "df3 = pd.DataFrame(target_rewards)\n",
    "df3.columns.name = 'episode'\n",
    "df3.index.name = 'learner'\n",
    "df3 = df3.stack().rename('returns').to_frame()\n",
    "\n",
    "sns.lineplot( data=df2, x='episode', y='returns');\n",
    "plt.title('The mean reward of an episode goes up with episode number')\n",
    "plt.show()\n",
    "print('Welp at least there is some indication of learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df2.query('episode==1999').reset_index().drop(columns=['episode'])\n",
    "c1['condition']='behavior policy'\n",
    "c2 = df3.droplevel('episode').reset_index()\n",
    "c2['condition']='target policy'\n",
    "\n",
    "compare = pd.concat( [c1, c2], ignore_index=True)\n",
    "sns.boxplot(data=compare, x='condition', y='returns');\n",
    "plt.title('Target greedy policy is median better than e-greedy behavior policy\\nbut way more variable');\n",
    "\n",
    "print(compare.drop(columns=['learner']).groupby('condition').agg(stats_list))\n",
    "plt.show()\n",
    "print('Here\\'s more evidence that the training duration is too short... while target policy is better than the behavior at the median it has some spectacularly bad learners at 2500 episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_track(track, states=None):\n",
    "    # Get a copy of the course layout and the current position.\n",
    "    course_layout = track.course.copy()\n",
    "\n",
    "    if not states:\n",
    "        states = [np.concatenate(track.get_state())]\n",
    "\n",
    "    # Set up the figure.\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Plot the course as a heatmap.\n",
    "    # Note: We flip the image vertically to match the coordinate system.\n",
    "    plt.imshow(np.flipud(course_layout.T), cmap=\"hot\", interpolation=\"nearest\")\n",
    "    plt.title(\"One episode, marker shows position, arrow shows velocity\")\n",
    "\n",
    "    for astate in states:\n",
    "        # Transform the current position to match the flipud effect.\n",
    "        # x coordinate remains the same; y coordinate is flipped.\n",
    "        s = astate[0]\n",
    "        \n",
    "        s1T = course_layout.shape[1] - 1 - s[1]\n",
    "\n",
    "        # Overlay the current position.\n",
    "        plt.plot(s[0], s1T, \"bo\", markersize=10)\n",
    "        \n",
    "        plt.arrow(\n",
    "            s[0],\n",
    "            s1T,\n",
    "            s[2],\n",
    "            -s[3],\n",
    "            color=\"cyan\",\n",
    "            head_width=0.2,\n",
    "            length_includes_head=True,\n",
    "            zorder=100\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to use this technique to explore solutions you train!\n",
    "\n",
    "plot_track(MC.env, episode)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On policy updating\n",
    "\n",
    "OK lets see if there's any difference when we use on an on-policy e-greedy first visit algorithm to learn.\n",
    "\n",
    "Warning (at least my implementation) is extremely slow!  Even downgrading this to only 5 trials of 2000 episodes each this is going to take 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) # replicability \n",
    "\n",
    "steps_on = []\n",
    "behavior_rewards_on = []\n",
    "target_rewards_on = []\n",
    "trials = 5\n",
    "for j in range(trials):\n",
    "    MC = MonteCarloControl(RaceTrack(tiny_course), max_episode_size=250)\n",
    "    this_steps = []\n",
    "    this_rewards = []\n",
    "    for k in tqdm(range(1000), desc='trial {} of {}'.format(j,trials)):\n",
    "        episode = MC.generate_egreedy_episode()\n",
    "        this_rewards.append( pd.DataFrame(episode).iloc[:,-1].sum() )\n",
    "        this_steps.append(len(episode))\n",
    "        MC.update_onpolicy(episode)\n",
    "    steps_on.append(this_steps)\n",
    "    behavior_rewards_on.append(this_rewards)\n",
    "    episode = MC.generate_greedy_episode()\n",
    "    target_rewards_on.append( pd.DataFrame(episode).iloc[:,-1].sum() )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_on = pd.DataFrame(behavior_rewards_on)\n",
    "df2_on.columns.name = 'episode'\n",
    "df2_on.index.name = 'learner'\n",
    "df2_on = df2_on.stack().rename('returns').to_frame()\n",
    "\n",
    "\n",
    "df3_on = pd.DataFrame(target_rewards_on)\n",
    "df3_on.columns.name = 'episode'\n",
    "df3_on.index.name = 'learner'\n",
    "df3_on = df3_on.stack().rename('returns').to_frame()\n",
    "\n",
    "sns.lineplot( data=df2_on, x='episode', y='returns');\n",
    "plt.title('The mean reward of an episode doesn\\'t improve with episode number')\n",
    "plt.show()\n",
    "print('NO indication of learning.  Perhaps we need way more episodes than I am willing to try?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
